{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unittest\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "\tdef __init__(self):\n",
    "\t\tself.total_sample = 0\n",
    "\t\tself.key_values = {}\n",
    "\n",
    "\tdef __call__(self, n_sample, **kwargs):\n",
    "\t\tfor k, v in kwargs.items():\n",
    "\t\t\tif k not in self.key_values:\n",
    "\t\t\t\tself.key_values[k] = v\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.key_values[k] += v\n",
    "\t\tself.total_sample += n_sample\n",
    "\n",
    "\tdef mean(self, key):\n",
    "\t\treturn self.key_values[key] / self.total_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "\tdef __init__(self, n_in, n_out, activation=None):\n",
    "\t\tself.n_in = n_in\n",
    "\t\tself.n_out = n_out\n",
    "\t\tself.activation = activation\n",
    "\t\tself.W = np.random.normal(scale=1.0/math.sqrt(n_in), size=(n_in, n_out))\n",
    "\t\tself.dW = np.zeros(shape=(n_in, n_out))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef stable_sigmoid(x):\n",
    "\t\tret = np.zeros_like(x)\n",
    "\t\tret[x >= 0] = 1 / (1 + np.exp(-x[x >= 0])) # 1 / (1+e^-x)\n",
    "\t\tret[x < 0] = np.exp(x[x < 0])\n",
    "\t\tret[x < 0] = ret[x < 0] / (1 + ret[x < 0]) # e^x / (e^x + 1)\n",
    "\t\treturn ret\n",
    "\n",
    "\tdef __activation(self, a):\n",
    "\t\tif self.activation is None:\n",
    "\t\t\tf = a.copy()\n",
    "\t\telif self.activation == \"sigmoid\":\n",
    "\t\t\tf = self.stable_sigmoid(a) #1.0/(1+np.exp(-self.a))\n",
    "\t\telif self.activation == \"relu\":\n",
    "\t\t\tf = a.copy()\n",
    "\t\t\tf[a < 0] = 0\n",
    "\t\telse:\n",
    "\t\t\traise NotImplementedError(f\"NotImplementedError FC.forward activation={self.activation}\")\n",
    "\t\treturn f\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# x: N x n_in\n",
    "\t\t# save computation for backward phase\n",
    "\t\tself.x = x.copy()\n",
    "\t\tself.a = np.matmul(x, self.W)\n",
    "\t\tself.f = self.__activation(self.a)\n",
    "\t\treturn self.f\n",
    "\n",
    "\tdef __deactivation(self, df, f, a):\n",
    "\t\tif self.activation is None:\n",
    "\t\t\tda = df.copy() # N x n_out\n",
    "\t\telif self.activation == \"sigmoid\":\n",
    "\t\t\tda = f*(1-f)*df # N x n_out\n",
    "\t\telif self.activation == \"relu\":\n",
    "\t\t\tda = df.copy()\n",
    "\t\t\tda[a < 0] = 0\n",
    "\t\treturn da\n",
    "\n",
    "\tdef backward(self, df):\n",
    "\t\t## df: N x n_out\n",
    "    \t## use pre-compute self.x, self.a and self.f to compute dx and dW\n",
    "\t\tda = self.__deactivation(df, self.f, self.a)\n",
    "\t\tself.dW = np.einsum('ij,ik->jk', self.x, da) # nhân từng hàng x với ma trận da, sau đó cộng tất cả giá trị lại với nhau\n",
    "\t\tself.dx = np.matmul(da, self.W.T) # N x n_in\n",
    "\t\tself.df = df\n",
    "\t\tself.da = da\n",
    "\t\treturn self.dx\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn [self.W]\n",
    "\n",
    "\tdef grads(self):\n",
    "\t\treturn [self.dW]\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tpass\n",
    "  \n",
    "\tdef eval(self):\n",
    "\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import backward\n",
    "\n",
    "\n",
    "class TestFCMethods(unittest.TestCase):\n",
    "\tdef test_fc_init(self):\n",
    "\t\tinput, output = 4, 5\n",
    "\t\tactivation = \"sigmoid\"\n",
    "\t\tfc = FC(input, output, activation=activation)\n",
    "\t\tself.assertEqual(fc.n_in, input)\n",
    "\t\tself.assertEqual(fc.n_out, output)\n",
    "\t\tself.assertEqual(fc.W.shape, (input, output))\n",
    "\t\tself.assertEqual(fc.dW.shape, (input, output))\n",
    "\t\tself.assertEqual(fc.dW.shape, (input, output))\n",
    "\t\tself.assertEqual(fc.activation, activation)\n",
    "\n",
    "\tdef test_fc_forward(self):\n",
    "\t\tx = np.zeros((3, 5), dtype=np.float32)\n",
    "\t\tfc = FC(5, 2, \"sigmoid\")\n",
    "\t\ty = fc.forward(x)\n",
    "\t\terror = np.sum(np.abs(y - np.ones((3,2))*0.5))\n",
    "\t\tself.assertEqual(y.shape, (3,2))\n",
    "\t\tself.assertLess(error, 1e-6)\n",
    "\n",
    "\tdef test_fc_forward_identity(self):\n",
    "\t\tx = np.zeros((4, 6), dtype=np.float32)\n",
    "\t\tfc = FC(6, 7)\n",
    "\t\ty = fc.forward(x)\n",
    "\t\terror = np.sum(np.abs(y - np.zeros((4, 7))))\n",
    "\t\tself.assertEqual(y.shape, (4, 7))\n",
    "\t\tself.assertLess(error, 1e-6)\n",
    "\n",
    "\tdef test_fc_backward_identity(self):\n",
    "\t\tx = np.zeros((3, 5), dtype=np.float32)\n",
    "\t\tfc = FC(5, 2, activation=None)\n",
    "\t\ty = fc.forward(x)\n",
    "\t\tdx = fc.backward(np.ones_like(y))\n",
    "\t\t# exercise: should add test on error of dx\n",
    "\t\tself.assertEqual(dx.shape, x.shape)\n",
    "\t\tself.assertEqual(fc.dW.shape, fc.W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\tdef __init__(self, n_in, hiddens, activation=\"sigmoid\", last_layer_linear=True):\n",
    "\t\tself.n_in = n_in\n",
    "\t\tself.hiddens = hiddens\n",
    "\t\tself.layers = [\n",
    "\t\t\t# use sigmoid activation for hidden layers\n",
    "\t\t\t# use linear activation for output layer\n",
    "\t\t\tFC(n_in=hiddens[i-1] if i > 0 else n_in,\n",
    "\t\t\tn_out=hiddens[i],\n",
    "\t\t\tactivation=activation if (i < len(hiddens)-1) or (not last_layer_linear) else None\n",
    "\t\t\t)\n",
    "\t\t\tfor i in range(len(hiddens))\n",
    "\t\t]\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = x\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tout = layer.forward(out)\n",
    "\t\treturn out\n",
    "\n",
    "\tdef backward(self, dout):\n",
    "\t\tfor layer in self.layers[::-1]:\n",
    "\t\t\tdout = layer.backward(dout)\n",
    "\t\treturn dout\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn sum([layer.parameters() for layer in self.layers], [])\n",
    "\t\n",
    "\tdef grads(self):\n",
    "\t\treturn sum([layer.grads() for layer in self.layers], [])\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tlayer.train()\n",
    "\n",
    "\tdef eval(self):\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tlayer.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for Multi-layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMLPMethods(unittest.TestCase):\n",
    "\tdef test_mlp_init(self):\n",
    "\t\tmodel = MLP(n_in=10, hiddens=[5, 2, 3])\n",
    "\t\tself.assertEqual(model.n_in, 10)\n",
    "\t\tself.assertEqual(model.hiddens, [5, 2, 3])\n",
    "\t\tself.assertEqual(model.layers[0].n_in, 10)\n",
    "\t\tself.assertEqual(model.layers[0].n_out, 5)\n",
    "\t\tself.assertEqual(model.layers[1].n_in, 5)\n",
    "\t\tself.assertEqual(model.layers[1].n_out, 2)\n",
    "\t\tself.assertEqual(model.layers[2].n_in, 2)\n",
    "\t\tself.assertEqual(model.layers[2].n_out, 3)\n",
    "\n",
    "\tdef test_mlp_forward(self):\n",
    "\t\tmodel = MLP(n_in=10, hiddens=[5, 2])\n",
    "\t\tx = np.zeros((3, 10), dtype=np.float32)\n",
    "\t\ty = model.forward(x)\n",
    "\t\tself.assertEqual(y.shape, (3, 2))\n",
    "\n",
    "\tdef test_mlp_backward(self):\n",
    "\t\tmodel = MLP(n_in=10, hiddens=[5, 2])\n",
    "\t\tx = np.zeros((3, 10), dtype=np.float32)\n",
    "\t\ty = model.forward(x)\n",
    "\t\tdx = model.backward(np.ones_like(y))\n",
    "\t\tself.assertEqual(dx.shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "\tdef __init__(self, p):\n",
    "\t\tself.p = p\n",
    "\t\tself.is_train = True\n",
    "\t\n",
    "\tdef parameters(self):\n",
    "\t\treturn []\n",
    "\n",
    "\tdef grads(self):\n",
    "\t\treturn []\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tif self.is_train:\n",
    "\t\t\tself.mask = np.random.rand(*x.shape) < self.p\n",
    "\t\t\tself.x = x\n",
    "\t\t\ty = x.copy()\n",
    "\t\t\ty[self.mask] = 0\n",
    "\t\telse:\n",
    "\t\t\ty = x * (1-self.p)\n",
    "\t\treturn y\n",
    "\n",
    "\tdef backward(self, dy):\n",
    "\t\tdx = dy.copy()\n",
    "\t\tdx[self.mask] = 0\n",
    "\t\treturn dx\n",
    "\t\n",
    "\tdef train(self):\n",
    "\t\tself.is_train = True\n",
    "\n",
    "\tdef eval(self):\n",
    "\t\tself.is_train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDropoutMethods(unittest.TestCase):\n",
    "\tdef test_dropout_init(self):\n",
    "\t\tdo = Dropout(p=0.2)\n",
    "\t\tself.assertEqual(do.p, 0.2)\n",
    "\t\tself.assertEqual(do.parameters(), [])\n",
    "\t\tself.assertEqual(do.grads(), [])\n",
    "\n",
    "\tdef test_dropout_forward(self):\n",
    "\t\tdo = Dropout(p=0.30)\n",
    "\t\tnp.random.seed(42)\n",
    "\t\tx = np.ones((10, 10))\n",
    "\t\ty = do.forward(x)\n",
    "\t\tcount_zero = np.sum(y==0)\n",
    "\t\tself.assertEqual(count_zero, 34)\n",
    "\n",
    "\tdef test_dropout_backward(self):\n",
    "\t\tdo = Dropout(p=0.30)\n",
    "\t\tnp.random.seed(42)\n",
    "\t\tx = np.ones((10, 10))*2\n",
    "\t\ty = do.forward(x)\n",
    "\t\tdx = do.backward(np.ones_like(y))\n",
    "\t\tcount_zero = np.sum(dx==0)\n",
    "\t\tself.assertEqual(count_zero, 34)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock:\n",
    "\tdef __init__(self, n_in, hiddens, activation=\"sigmoid\", last_layer_linear=False, dropout=-1.0):\n",
    "\t\t## initialize layers\n",
    "\t\tn_out = hiddens[-1]\n",
    "\t\tself.input_fc = FC(n_in, hiddens[0], activation=activation)\n",
    "\t\tself.block = MLP(n_in=hiddens[0], hiddens=hiddens[1:], activation=activation, last_layer_linear=last_layer_linear)\n",
    "\t\tself.skip = FC(n_in=hiddens[0], n_out=n_out, activation=activation) if hiddens[0] != n_out else None\n",
    "\t\tself.dropout = Dropout(p=dropout) if dropout > 0 else None\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.input_fc.forward(x)\n",
    "\t\tblock_out = self.block.forward(x)\n",
    "\t\tskip_out = self.skip.forward(x) if self.skip is not None else x\n",
    "\t\tout = block_out + skip_out\n",
    "\t\tout = self.dropout.forward(out) if self.dropout is not None else out\n",
    "\t\treturn out\n",
    "\n",
    "\tdef backward(self, dout):\n",
    "\t\tdout = self.dropout.backward(dout) if self.dropout is not None else dout\n",
    "\t\td_block_in = self.block.backward(dout)\n",
    "\t\td_skip_in  = self.skip.backward(dout) if self.skip is not None else dout\n",
    "\t\tdx = d_block_in + d_skip_in\n",
    "\t\tdx = self.input_fc.backward(dx)\n",
    "\t\treturn dx\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn self.input_fc.parameters() + self.block.parameters() + (self.skip.parameters() if self.skip is not None else [])\n",
    "\n",
    "\tdef grads(self):\n",
    "\t\treturn self.input_fc.grads() + self.block.grads() + (self.skip.grads() if self.skip is not None else [])\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tself.input_fc.train()\n",
    "\t\tself.block.train()\n",
    "\t\tif self.skip is not None:\n",
    "\t\t\tself.skip.train()\n",
    "\t\tif self.dropout is not None:\n",
    "\t\t\tself.dropout.train()\n",
    "\n",
    "\tdef eval(self):\n",
    "\t\tself.input_fc.eval()\n",
    "\t\tself.block.eval()\n",
    "\t\tif self.skip is not None:\n",
    "\t\t\tself.skip.eval()\n",
    "\t\tif self.dropout is not None:\n",
    "\t\t\tself.dropout.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestResBlockMethods(unittest.TestCase):\n",
    "\tdef test_res_init(self):\n",
    "\t\tmodel = ResidualBlock(n_in=10, hiddens=[5, 6, 2])\n",
    "\t\tfc = model.input_fc\n",
    "\t\tlayer0 = model.block.layers[0]\n",
    "\t\tlayer1 = model.block.layers[1]\n",
    "\t\tskip = model.skip\n",
    "\t\tself.assertEqual((fc.n_in, fc.n_out), (10,5))\n",
    "\t\tself.assertEqual(model.block.n_in, 5)\n",
    "\t\tself.assertEqual(model.block.hiddens, [6, 2])\n",
    "\t\tself.assertEqual(layer0.n_in, 5)\n",
    "\t\tself.assertEqual(layer0.n_out, 6)\n",
    "\t\tself.assertEqual(layer1.n_in, 6)\n",
    "\t\tself.assertEqual(layer1.n_out, 2)\n",
    "\t\tself.assertEqual(skip.n_in, 5)\n",
    "\t\tself.assertEqual(skip.n_out, 2)\n",
    "\n",
    "\tdef test_res_identity(self):\n",
    "\t\tmodel = ResidualBlock(n_in=10, hiddens=[5, 6, 5])\n",
    "\t\tfc = model.input_fc\n",
    "\t\tlayer0 = model.block.layers[0]\n",
    "\t\tlayer1 = model.block.layers[1]\n",
    "\t\tskip = model.skip\n",
    "\t\tself.assertEqual((fc.n_in, fc.n_out), (10,5))\n",
    "\t\tself.assertEqual(model.block.n_in, 5)\n",
    "\t\tself.assertEqual(model.block.hiddens, [6, 5])\n",
    "\t\tself.assertEqual(layer0.n_in, 5)\n",
    "\t\tself.assertEqual(layer0.n_out, 6)\n",
    "\t\tself.assertEqual(layer1.n_in, 6)\n",
    "\t\tself.assertEqual(layer1.n_out, 5)\n",
    "\t\tself.assertIsNone(skip, None)\n",
    "\n",
    "\tdef test_res_forward(self):\n",
    "\t\tmodel = ResidualBlock(n_in=10, hiddens=[5, 2])\n",
    "\t\tx = np.zeros((3, 10), dtype=np.float32)\n",
    "\t\ty = model.forward(x)\n",
    "\t\tself.assertEqual(y.shape, (3, 2))\n",
    "\n",
    "\tdef test_res_backward(self):\n",
    "\t\tmodel = ResidualBlock(n_in=10, hiddens=[5, 2])\n",
    "\t\tx = np.zeros((3, 10), dtype=np.float32)\n",
    "\t\ty = model.forward(x)\n",
    "\t\tdx = model.backward(np.ones_like(y))\n",
    "\t\tself.assertEqual(dx.shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "\tdef __init__(self, n_in, blocks_hiddens, n_out, activation=\"sigmoid\", dropout=None):\n",
    "\t\tself.n_in = n_in\n",
    "\t\tself.blocks_hiddens = blocks_hiddens\n",
    "\t\tself.n_out = n_out\t\t\n",
    "\t\tself.blocks = [\n",
    "\t\t               ResidualBlock(\n",
    "\t\t                   n_in=blocks_hiddens[i-1][-1] if i > 0 else n_in,\n",
    "\t\t                   hiddens = blocks_hiddens[i],\n",
    "\t\t                   activation=activation,\n",
    "\t\t                   last_layer_linear=False,\n",
    "\t\t                   dropout=-1 if dropout is None else dropout[i]\n",
    "\t\t                   )\n",
    "\t\t               for i in range(len(blocks_hiddens))\n",
    "\t\t               ]\n",
    "\t\tself.fc = FC(n_in=blocks_hiddens[-1][-1], n_out=n_out)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = x\n",
    "\t\tfor block in self.blocks:\n",
    "\t\t\tout = block.forward(out)\n",
    "\t\tout = self.fc.forward(out)\n",
    "\t\treturn out\n",
    "\n",
    "\tdef backward(self, dout):\n",
    "\t\tdout = self.fc.backward(dout)\n",
    "\t\tfor block in self.blocks[::-1]:\n",
    "\t\t\tdout = block.backward(dout)\n",
    "\t\treturn dout\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn sum([block.parameters() for block in self.blocks], []) + self.fc.parameters()\n",
    "\n",
    "\tdef grads(self):\n",
    "\t\treturn sum([block.grads() for block in self.blocks], []) + self.fc.grads()\n",
    "\n",
    "\tdef train(self):\n",
    "\t\tfor layer in self.blocks+[self.fc]:\n",
    "\t\t\tlayer.train()\n",
    "\n",
    "\tdef eval(self):\n",
    "\t\tfor layer in self.blocks+[self.fc]:\n",
    "\t\t\tlayer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "\t@staticmethod\n",
    "\tdef stable_softmax(X):\n",
    "\t\texps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "\t\treturn exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "\tdef forward(self, ypred, ytrue):\n",
    "\t\t## ypred: N x n_out (logit values)\n",
    "\t\t## ytrue: N (class label: int value in 0-->n_out-1)\n",
    "\t\t## should return - sum_i sum_c y_ic \\log mu_ic\n",
    "\t\tn, n_out = ypred.shape\n",
    "\t\tself.ypred = ypred\n",
    "\t\tself.ytrue = ytrue\n",
    "\t\tself.mu = self.stable_softmax(ypred)\n",
    "\t\tmu_ytrue = self.mu[range(n), ytrue]\n",
    "\t\tmu_ytrue[mu_ytrue < 1e-8] = 1e-8\n",
    "\t\tloss = np.sum(-np.log(mu_ytrue))\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef backward(self):\n",
    "\t\t## should return d_ypred, derivative of loss on ypred\n",
    "\t\t## d_ypred = mu - y (one-hot encoding of ytrue)\n",
    "\t\tn, n_out = self.ypred.shape\n",
    "\t\td_ypred = self.mu.copy()\n",
    "\t\td_ypred[range(n), self.ytrue] -= 1\n",
    "\t\treturn d_ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCEMethods(unittest.TestCase):\n",
    "\tdef test_ce_forward(self):\n",
    "\t\typred = np.zeros((10, 5))\n",
    "\t\tytrue = np.array([0,1,2,3,4,0,1,2,3,4], dtype=int)\n",
    "\t\tce = CrossEntropyLoss()\n",
    "\t\tloss = ce.forward(ypred, ytrue)\n",
    "\t\tself.assertAlmostEqual(loss, -10*math.log(1/5))\n",
    "\n",
    "\tdef test_ce_backward(self):\n",
    "\t\typred = np.zeros((10, 5))\n",
    "\t\tytrue = np.array([0,1,2,3,4,0,1,2,3,4], dtype=int)\n",
    "\t\tce = CrossEntropyLoss()\n",
    "\t\tloss = ce.forward(ypred, ytrue)\n",
    "\t\td_ypred = ce.backward()\n",
    "\t\tdesired = np.ones((10,5))*0.2\n",
    "\t\tdesired[range(10), ytrue] -= 1\n",
    "\t\terror = np.sum(np.abs(d_ypred-desired))\n",
    "\t\tself.assertAlmostEqual(error, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SGDOptimizer:\n",
    "\tdef __init__(self, model, learning_rate, regularization=0.0):\n",
    "\t\tself.model = model\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.regularization = regularization\n",
    "\t\tself.current_step = 0\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn self.model.parameters()\n",
    "\n",
    "\tdef grads(self):\n",
    "\t\treturn self.model.grads()\n",
    "\n",
    "\tdef zero_grad(self):\n",
    "\t\tfor g in self.grads():\n",
    "\t\t\tg.fill(0)\n",
    "\n",
    "\tdef step(self):\n",
    "\t\t## input is the derivative of loss function on the output of the model\n",
    "\t\t## dW has been computed by backward functions\n",
    "\t\t## perform a gradient step W = W - 1/sqrt(t) lambda dW\n",
    "\t\t## the learning rate is reduced over time for convergence\n",
    "\t\tself.current_step += 1\n",
    "\t\tfor p, g in zip(self.parameters(), self.grads()):\n",
    "\t\t\tg = self.regularization*p + g\n",
    "\t\t\tg = np.clip(g, -1, 1)\n",
    "\t\t\tp -= 1.0 / math.sqrt(self.current_step)*self.learning_rate*g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class AdaGradOptimizer:\n",
    "\tdef __init__(self, model, learning_rate, regularization=0.0):\n",
    "\t\tself.model = model\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.regularization = regularization\n",
    "\t\tself.current_step = 0\n",
    "\t\tself.sum_grad = [np.zeros_like(p) for p in model.parameters()]\n",
    "\n",
    "\tdef parameters(self):\n",
    "\t\treturn self.model.parameters()\n",
    "\n",
    "\tdef grads(self):\n",
    "\t\treturn self.model.grads()\n",
    "\n",
    "\tdef zero_grad(self):\n",
    "\t\tfor g in self.grads():\n",
    "\t\t\tg.fill(0)\n",
    "\n",
    "\tdef step(self):\n",
    "\t\t## input is the derivative of loss function on the output of the model\n",
    "\t\t## dW has been computed by backward functions\n",
    "\t\t## perform a scaled gradient step W = W - 1/sqrt(G+eps) lambda dW\n",
    "\t\t## the learning rate is reduced over time by square root of sum of gradient squares\n",
    "\n",
    "\t\teps = 1e-8\n",
    "\t\tfor p, g, G in zip(self.parameters(), self.grads(), self.sum_grad):\n",
    "\t\t\tg = self.regularization*p + g\n",
    "\t\t\tg = np.clip(g, -1, 1)\n",
    "\t\t\tG += (g*g) ## sum of gradient squares until current step\n",
    "\n",
    "\t\t\tp -= (self.learning_rate / np.sqrt(G+eps) * g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSGDMethods(unittest.TestCase):\n",
    "\tdef test_sgd_init(self):\n",
    "\t\tmodel = MLP(n_in=10, hiddens=[5, 2])\n",
    "\t\tsgd = SGDOptimizer(model, learning_rate=0.2, regularization=0.1)\n",
    "\t\tparam = sgd.parameters()\n",
    "\t\tgrad = sgd.grads()\n",
    "    \n",
    "\t\tfor p, g in zip(param, grad):\n",
    "\t\t\tself.assertEqual(p.shape, g.shape)\n",
    "\t\tself.assertEqual(sgd.learning_rate, 0.2)\n",
    "\t\tself.assertEqual(sgd.regularization, 0.1)\n",
    "\n",
    "\tdef test_sgd_zero_grad(self):\n",
    "\t\tmodel = MLP(n_in=10, hiddens=[5, 2])\n",
    "\t\tsgd = SGDOptimizer(model, learning_rate=0.2)\n",
    "\t\tsgd.zero_grad()\n",
    "\n",
    "\t\tfor g in sgd.grads():\n",
    "\t\t\tself.assertAlmostEqual(np.sum(np.abs(g)), 0)\n",
    "\n",
    "\tdef test_sgd_step(self):\n",
    "\t\tmodel = MLP(n_in=10, hiddens=[5, 2])\n",
    "\t\tsgd = SGDOptimizer(model, learning_rate=0.2)\n",
    "\t\tloss_func = CrossEntropyLoss()\n",
    "\t\t\t\n",
    "\t\tx = np.zeros((3, 10), dtype=np.float32)\n",
    "\t\tytrue = np.array([0,1,0], dtype=int)\n",
    "\n",
    "\t\typred = model.forward(x)\n",
    "\t\tloss = loss_func.forward(ypred, ytrue)\n",
    "\n",
    "\t\tsgd.zero_grad()\n",
    "\t\tdout = loss_func.backward()\n",
    "\t\tdx = model.backward(dout)\n",
    "\t\tsgd.step()\n",
    "\n",
    "\tdef test_sgd_n_step(self):\n",
    "\t\tmodel = MLP(n_in=10, hiddens=[5, 2])\n",
    "\t\tsgd = SGDOptimizer(model, learning_rate=0.02)\n",
    "\t\tloss_func = CrossEntropyLoss()\n",
    "\t\tn_step = 10\n",
    "\t\t\t\n",
    "\t\tx = np.zeros((3, 10), dtype=np.float32)\n",
    "\t\tytrue = np.array([0,1,0], dtype=int)\n",
    "\n",
    "\t\tprint()\n",
    "\t\tfor step in range(n_step):\n",
    "\t\t\tmodel.train()\n",
    "\t\t\typred = model.forward(x)\n",
    "\t\t\tloss = loss_func.forward(ypred, ytrue)\n",
    "\n",
    "\t\t\tprint(f\"step {step} {loss:.4f}\")\n",
    "\t\t\tif step > 0:\n",
    "\t\t\t\tself.assertLess(loss, old_loss) ## SGD step reduces loss function\n",
    "\t\t\told_loss = loss\n",
    "\n",
    "\t\t\tsgd.zero_grad()\n",
    "\t\t\tdout = loss_func.backward()\n",
    "\t\t\tdx = model.backward(dout)\n",
    "\t\t\tsgd.step()\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_ce_backward (__main__.TestCEMethods) ... ok\n",
      "test_ce_forward (__main__.TestCEMethods) ... ok\n",
      "test_dropout_backward (__main__.TestDropoutMethods) ... ok\n",
      "test_dropout_forward (__main__.TestDropoutMethods) ... ok\n",
      "test_dropout_init (__main__.TestDropoutMethods) ... ok\n",
      "test_fc_backward_identity (__main__.TestFCMethods) ... ok\n",
      "test_fc_forward (__main__.TestFCMethods) ... ok\n",
      "test_fc_forward_identity (__main__.TestFCMethods) ... ok\n",
      "test_fc_init (__main__.TestFCMethods) ... ok\n",
      "test_mlp_backward (__main__.TestMLPMethods) ... ok\n",
      "test_mlp_forward (__main__.TestMLPMethods) ... ok\n",
      "test_mlp_init (__main__.TestMLPMethods) ... ok\n",
      "test_res_backward (__main__.TestResBlockMethods) ... ok\n",
      "test_res_forward (__main__.TestResBlockMethods) ... ok\n",
      "test_res_identity (__main__.TestResBlockMethods) ... ok\n",
      "test_res_init (__main__.TestResBlockMethods) ... ok\n",
      "test_sgd_init (__main__.TestSGDMethods) ... ok\n",
      "test_sgd_n_step (__main__.TestSGDMethods) ... ok\n",
      "test_sgd_step (__main__.TestSGDMethods) ... ok\n",
      "test_sgd_zero_grad (__main__.TestSGDMethods) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 20 tests in 0.013s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "step 0 2.1211\n",
      "step 1 2.1102\n",
      "step 2 2.1029\n",
      "step 3 2.0970\n",
      "step 4 2.0921\n",
      "step 5 2.0878\n",
      "step 6 2.0839\n",
      "step 7 2.0804\n",
      "step 8 2.0772\n",
      "step 9 2.0742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1f5ab877ca0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d533614f4d57ff4a1b942d0131a5a6cbb98dc8aa993cf976d4f887313b883c27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
